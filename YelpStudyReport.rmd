???---
title: "Using Yelp review text to predict user rating"
author: "Stéphane Nyombayire"
date: "November 17, 2015"
output: word_document
---
## Introduction

This project aims at analyzing •[Yelp review data set](http://www.yelp.com/dataset_challenge) to understand whether one could infer the rating based on text analysis of the verbatim review text. The rationale behind this study is both an academic exercise on natural language techniques for deriving insights from text, but more importantly this could help Yelp determine the importance of the verbiage provided by the users' review text to assess whether there is an incremental benefit of having the text for rating a business.

## Methods and Data

The dataset used is located •[here](https://www.yelp.com/dataset_challenge/dataset). 
```{r, echo=FALSE, cache=TRUE, warning= FALSE}
source("Utility.R")
library("ggplot2")
library("wordcloud")
require("topicmodels")
require(plyr)
require(rpart)
require(e1071)
require(caret)
require(jsonlite)
require('tm')
```
Loading data:
```{r, echo=TRUE, cache=TRUE,  eval=TRUE}
rFile = "yelp_academic_dataset_review.json"
dat <- fromJSON(sprintf("[%s]", paste(readLines(rFile), collapse=",")), flatten = TRUE)
```

We used the "yelp_academic_dataset_review.json" file. To read-in the file, we first flatten the data structure into a table for further processing. 
```{r, echo=TRUE, cache=TRUE}
str(dat)
```
Many of the techniques applied require to use a document-term matrix as input. To obtain such matrix we have processed each of the reviews to build a bag of words language model. To create this model we preprocessed each document in the corpus as follows:

  1. Remove non-writable characters.
  2. Strip extra white spaces.
  3. Lower case.
  4. Remove punctuation
  5. Remove numbers
  6. Stemming
  7. Stop words removal.

After that, each text was tokenized into unigrams, and the unigram frequencies were counted and stored into a document-term matrix of counts.This matrix will serve as the base of our modeling and analysis.

```{r, echo=FALSE, cache=TRUE, warning= FALSE}
source("Utility.R")
data = dat[sample(nrow(dat), 1000), ]
```

```{r, echo=TRUE, cache=TRUE, warning=FALSE}
matrix <- getDtm(data$text)
```
### Exploratory Analysis and Feature Engineering

#### Frequent Terms Classifier
In order to further understand our data set, let's look at the most frequently used terms using the matrix produced above:
```{r, echo=TRUE, cache=TRUE, eval= TRUE}
head(findFreqTerms(matrix, lowfreq=100), 10)
```

We noticed that there are words such as: best, nice, love, amazing, better, bad, pretty that are widely used and not too surprising given that these are individual reviews of business. This is already a hint at the need to evaluate sentiment for this feedback. We will get to it later. 

We can also plot word frequencies (we choose at least 300 mentions):
```{r, echo=FALSE, cache=TRUE, eval= TRUE}
freq <- sort(colSums(as.matrix(matrix)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
ggplot(subset(wf, freq > 300), aes(word, freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Alternatively, for better visual consumption we can use a word cloud:
```{r, echo=FALSE, cache=TRUE, eval= TRUE, warning= FALSE}
set.seed(123)
wordcloud(names(freq), freq, min.freq = 50, colors = brewer.pal(6, "Dark2"))
```

These terms based on frequency of occurence serve as the basis our initial data set that we will be using for modeling.

### Sentiment Analysis
Our next strategy with feature engineering would be to infer "sentiment" from each word in our matrix. We will score eac word based on the frequency of usage and as reference for positive, neutral and negative we use the following dictionary:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
lexicon <- read.csv("subjectivity.csv", head = FALSE, col.names = c('word', 'polarity', 'sentiment'))
str(lexicon)
```
We will run getSentiment to expand the list of predictors to include the sentiment score as follow:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
sentiment = getSentiment(matrix)
data$pScore = as.numeric(sentiment[, 1])
data$nScore = as.numeric(sentiment[, 2])
data$rScore = as.numeric(sentiment[,3])
data$sentiment = sentiment[, 4]
```
Now let's see how the sentiment analysis performed, if we assume negative to postive rating from 1 through 5
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
plot(prop.table(table(data$stars, data$sentiment ), 1), main = "Distribution of Rating per Sentiment score", xlab = "Rating", ylab = "Sentiment", color = "pink")
```

### Topic based modeling
To enhance our features for better modeling, in addition to term frequency and sentiment analysis we will also extract the key topics using Latent Dirichlet Allocation (i.e: LDA) using Gibbs method. Now for each word in our document matrix, we will compute the gamma distance to each topic as follow:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
lda <- LDA(matrix, method = "Gibbs", control = list(alpha = 0.3), 15)
gammaDF <- as.data.frame(lda@gamma)
names(gammaDF) <- terms(lda)
names(gammaDF)
df <- cbind(data, gammaDF)
```
Let's add the avgRating predictor - we want to examine how average behavior from users perform in our final model, We will also tally up all the votes in another column TotalVotes:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
dr = ddply(df,.(user_id),transform,avgRating = mean(stars))
dr$TotalVotes = dr$votes.funny + dr$votes.useful + dr$votes.cool
```
## Results
We will try two models for this classification problem. SVM (support vector machines) and Decision Tree classifier using RPart.
First, we split our data sets in training and validation data sets:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
d = subset(dr, select = -c(user_id,review_id, date, text, type, business_id))
d$sentiment = as.factor(d$sentiment)
d$stars = as.factor(d$stars)
inTraining = createDataPartition(d$stars, p = .75, list = FALSE)
training = d[ inTraining,]
testing = d[-inTraining,]
```
### SVM
Find below the result from SVM. The model performance is rather poor with accuracy of 38%.
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
svm.model <- svm(stars ~ ., data = training, cost = 100, gamma = 1, type = "C-classification") 
svm.pred <- predict(svm.model, testing[,-1])
confusionMatrix(svm.pred, testing[,1])["overall"]
```
### RPart
Decision tree model performs alot better acheiving 97% accuracy with very good recall and specificity as well. Kappa is also pretty high. 

```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
rpart.model = rpart(stars ~ ., data= training )
rpart.pred = predict(rpart.model, testing[, -1], type = "class")
confusionMatrix(rpart.pred, testing[,1])["overall"]
```

## Discussion
Going back to the original question: "Are we able to predict rating solely base on the review text?". The answer based on our analysis is yes. However, digging deeper our model perfoms rather well when only considering avgRating. Using the same RPart algorithm without average rating we notice a significant drop in accuracy to 39% accuracy. This shows that users dont really change their average behavior when it comes to rating!

Only AvgRating:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
rpart.model = rpart(stars ~ ., data= subset(training, select = c(stars, avgRating)))
rpart.pred = predict(rpart.model, testing[, -1], type = "class")
confusionMatrix(rpart.pred, testing[,1])["overall"]
```
No AvgRating:
```{r, echo=TRUE, cache=TRUE, eval= TRUE, warning= FALSE}
rpart.model = rpart(stars ~ ., data= subset(training, select = -c(avgRating)))
rpart.pred = predict(rpart.model, testing[, -1], type = "class")
confusionMatrix(rpart.pred, testing[,1])["overall"] 
```